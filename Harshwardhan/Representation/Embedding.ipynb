{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define columns to read\n",
    "columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)','Name']\n",
    "\n",
    "# Import all files\n",
    "df_1 = pd.read_excel('set1.xls', usecols=columns)\n",
    "df_2 = pd.read_excel('set2.xls', usecols=columns)\n",
    "df_3 = pd.read_excel('set3.xlsx', usecols=columns)\n",
    "df_5 = pd.read_csv('set5.csv', usecols=columns)\n",
    "df_6 = pd.read_excel('set6.xlsx', usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aea40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1['Name'].value_counts())\n",
    "print(df_2['Name'].value_counts())\n",
    "print(df_3['Name'].value_counts())\n",
    "print(df_5['Name'].value_counts())\n",
    "print(df_6['Name'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c805bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes vertically\n",
    "df_combined = pd.concat([df_1, df_2, df_3, df_5, df_6], ignore_index=True)\n",
    "\n",
    "# Display first few rows of combined dataframe\n",
    "df_combined.head()\n",
    "\n",
    "# Optional: check the total number of rows\n",
    "print(f\"Total number of rows: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546957ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('CombinedDeinterleaved.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d09241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# You can tune these parameters\n",
    "EMBEDDING_DIM = 128      # The size of the learned embedding vector\n",
    "MARGIN = 1.0             # The margin for the Triplet Loss\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "TEST_SPLIT_SIZE = 0.2    # 20% of data will be used for testing\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads PDW data from a CSV file and preprocesses it.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pdws_normalized (np.ndarray): Normalized array of PDW features.\n",
    "            - labels_numeric (np.ndarray): Array of numeric labels.\n",
    "            - label_mapping (dict): A dictionary mapping numeric labels back to original names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "        \n",
    "    print(f\"Loading data from '{file_path}'...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define feature and label columns\n",
    "    feature_columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)']\n",
    "    label_column = 'Name'\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in feature_columns + [label_column]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Error: CSV file must contain the column '{col}'.\")\n",
    "\n",
    "    # Extract features\n",
    "    pdws = df[feature_columns].values.astype(np.float32)\n",
    "\n",
    "    # Convert string labels to numeric labels\n",
    "    labels_numeric, unique_labels = pd.factorize(df[label_column])\n",
    "    label_mapping = {i: name for i, name in enumerate(unique_labels)}\n",
    "    \n",
    "    print(\"   Label mapping created:\")\n",
    "    for numeric_label, string_label in label_mapping.items():\n",
    "        print(f\"   {numeric_label} -> '{string_label}'\")\n",
    "\n",
    "    # Normalize all feature columns\n",
    "    pdws_normalized = np.copy(pdws)\n",
    "    for col in range(pdws_normalized.shape[1]):\n",
    "        mean = pdws_normalized[:, col].mean()\n",
    "        std = pdws_normalized[:, col].std()\n",
    "        if std > 0:\n",
    "            pdws_normalized[:, col] = (pdws_normalized[:, col] - mean) / std\n",
    "    \n",
    "    print(f\"   Successfully loaded and processed {len(df)} pulses.\")\n",
    "    return pdws_normalized, labels_numeric, label_mapping\n",
    "\n",
    "\n",
    "# --- 3. Triplet Dataset Class ---\n",
    "# This custom PyTorch Dataset creates triplets of single PDW vectors on-the-fly.\n",
    "class TripletPDWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for generating triplets of single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdws, labels):\n",
    "        self.pdws = pdws\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Group indices by label for efficient triplet mining\n",
    "        self.label_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "        if len(self.labels_set) < 2:\n",
    "            raise ValueError(\"The dataset must contain at least two different classes to create triplets.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pdws)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # --- Anchor Selection ---\n",
    "        anchor_idx = index\n",
    "        anchor_label = self.labels[anchor_idx]\n",
    "        anchor_pdw = torch.FloatTensor(self.pdws[anchor_idx])\n",
    "\n",
    "        # --- Positive Selection ---\n",
    "        # Randomly select another sample from the same class\n",
    "        positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_pdw = torch.FloatTensor(self.pdws[positive_idx])\n",
    "\n",
    "        # --- Negative Selection ---\n",
    "        # Randomly select a sample from a different class\n",
    "        negative_label = random.choice([l for l in self.labels_set if l != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative_pdw = torch.FloatTensor(self.pdws[negative_idx])\n",
    "\n",
    "        return anchor_pdw, positive_pdw, negative_pdw\n",
    "\n",
    "# --- 4. Model Architecture (The Encoder f(x)) ---\n",
    "# An ANN/MLP-based network to process single PDWs and output an embedding.\n",
    "class EmitterEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ANN/MLP-based encoder to generate embeddings from single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(EmitterEncoder, self).__init__()\n",
    "        # A simple multi-layer perceptron\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.net(x)\n",
    "        # Normalize the embedding to have unit length (improves stability)\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "# --- 5. Triplet Loss Function (Removed custom class) ---\n",
    "# We will now use the built-in PyTorch loss function nn.TripletMarginLoss\n",
    "\n",
    "# --- 6. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # !!! IMPORTANT !!!\n",
    "    # !!! CHANGE THIS PATH TO POINT TO YOUR CSV FILE !!!\n",
    "    # CSV_FILE_PATH = 'sampledpoints.csv' \n",
    "    CSV_FILE_PATH = 'CombinedDeinterleaved.csv' \n",
    "    \n",
    "    print(\"--- Emitter Classification Training using Siamese Triplet Loss (ANN Version) ---\")\n",
    "\n",
    "   \n",
    "    pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "    # Step 2: Split data into Training and Testing sets\n",
    "    print(f\"\\n2. Splitting data into {1-TEST_SPLIT_SIZE:.0%} training and {TEST_SPLIT_SIZE:.0%} testing...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pdws, labels, test_size=TEST_SPLIT_SIZE, random_state=42, stratify=labels\n",
    "    )\n",
    "    print(f\"   Training set size: {len(X_train)}\")\n",
    "    print(f\"   Testing set size: {len(X_test)}\")\n",
    "\n",
    "    # Step 3: Create Dataset and DataLoader for TRAINING data\n",
    "    print(\"\\n3. Creating Triplet Dataset and DataLoader for training...\")\n",
    "    train_dataset = TripletPDWDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Step 4: Initialize Model, Loss, and Optimizer\n",
    "    print(\"4. Initializing model, loss function, and optimizer...\")\n",
    "    input_dim = pdws.shape[1] \n",
    "    model = EmitterEncoder(input_dim=input_dim, hidden_dim=128, embedding_dim=EMBEDDING_DIM)\n",
    "    \n",
    "    # *** CHANGE: Use the built-in PyTorch TripletMarginLoss ***\n",
    "    criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Step 5: Training Loop with Loss Tracking\n",
    "    print(f\"5. Starting training for {EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize loss tracking\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            anchor_emb, positive_emb, negative_emb = model(anchor), model(positive), model(negative)\n",
    "            \n",
    "            # The call to the criterion remains the same\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"   Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training complete! ---\")\n",
    "\n",
    "    # Step 5.5: Plot Loss Curve\n",
    "    print(\"\\n5.5. Plotting training loss curve...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), epoch_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Training Loss Curve - Triplet Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Step 6: Evaluation\n",
    "    print(\"\\n6. Evaluating the model on the test set...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # A: Generate embeddings for the entire training set to define cluster centroids\n",
    "        train_embeddings = model(torch.FloatTensor(X_train)).numpy()\n",
    "        \n",
    "        # B: Calculate the centroid for each cluster from the training data\n",
    "        cluster_centroids = defaultdict(list)\n",
    "        for i, label in enumerate(y_train):\n",
    "            cluster_centroids[label].append(train_embeddings[i])\n",
    "        \n",
    "        for label in cluster_centroids:\n",
    "            cluster_centroids[label] = np.mean(cluster_centroids[label], axis=0)\n",
    "            \n",
    "        # C: Generate embeddings for the test set\n",
    "        test_embeddings = model(torch.FloatTensor(X_test)).numpy()\n",
    "        \n",
    "        # D: Predict the label for each test embedding by finding the closest centroid\n",
    "        predictions = []\n",
    "        for test_emb in test_embeddings:\n",
    "            distances = {label: np.linalg.norm(test_emb - centroid) for label, centroid in cluster_centroids.items()}\n",
    "            predicted_label = min(distances, key=distances.get)\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "        # E: Calculate and print accuracy and classification report\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"\\n   Overall Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        report_labels = sorted(list(label_mapping.keys()))\n",
    "        report_target_names = [label_mapping[lbl] for lbl in report_labels]\n",
    "        print(\"\\n   Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, labels=report_labels, target_names=report_target_names))\n",
    "\n",
    "\n",
    "    # Step 7: Visualization of TEST SET embeddings\n",
    "    print(\"\\n7. Visualizing the learned embedding space for the TEST SET using t-SNE...\")\n",
    "    # Use t-SNE to reduce dimensionality of test embeddings to 2D for plotting\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_test)-1), n_iter=1000)\n",
    "    embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Test Set Emitter Embeddings')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    legend_labels = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "    plt.legend(handles=scatter.legend_elements(num=len(legend_labels))[0], labels=legend_labels)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"--- Script finished. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# You can tune these parameters\n",
    "EMBEDDING_DIM = 128      # The size of the learned embedding vector\n",
    "MARGIN = 1.0             # The margin for the Triplet Loss\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "TEST_SPLIT_SIZE = 0.2    # 20% of data will be used for testing\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads PDW data from a CSV file and preprocesses it.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pdws_normalized (np.ndarray): Normalized array of PDW features.\n",
    "            - labels_numeric (np.ndarray): Array of numeric labels.\n",
    "            - label_mapping (dict): A dictionary mapping numeric labels back to original names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "        \n",
    "    print(f\"Loading data from '{file_path}'...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define feature and label columns\n",
    "    feature_columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)']\n",
    "    label_column = 'Name'\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in feature_columns + [label_column]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Error: CSV file must contain the column '{col}'.\")\n",
    "\n",
    "    # Extract features\n",
    "    pdws = df[feature_columns].values.astype(np.float32)\n",
    "\n",
    "    # Convert string labels to numeric labels\n",
    "    labels_numeric, unique_labels = pd.factorize(df[label_column])\n",
    "    label_mapping = {i: name for i, name in enumerate(unique_labels)}\n",
    "    \n",
    "    print(\"   Label mapping created:\")\n",
    "    for numeric_label, string_label in label_mapping.items():\n",
    "        print(f\"   {numeric_label} -> '{string_label}'\")\n",
    "\n",
    "    # Normalize all feature columns\n",
    "    pdws_normalized = np.copy(pdws)\n",
    "    for col in range(pdws_normalized.shape[1]):\n",
    "        mean = pdws_normalized[:, col].mean()\n",
    "        std = pdws_normalized[:, col].std()\n",
    "        if std > 0:\n",
    "            pdws_normalized[:, col] = (pdws_normalized[:, col] - mean) / std\n",
    "    \n",
    "    print(f\"   Successfully loaded and processed {len(df)} pulses.\")\n",
    "    return pdws_normalized, labels_numeric, label_mapping\n",
    "\n",
    "\n",
    "# --- 3. Triplet Dataset Class ---\n",
    "# This custom PyTorch Dataset creates triplets of single PDW vectors on-the-fly.\n",
    "class TripletPDWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for generating triplets of single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdws, labels):\n",
    "        self.pdws = pdws\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Group indices by label for efficient triplet mining\n",
    "        self.label_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "        if len(self.labels_set) < 2:\n",
    "            raise ValueError(\"The dataset must contain at least two different classes to create triplets.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pdws)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # --- Anchor Selection ---\n",
    "        anchor_idx = index\n",
    "        anchor_label = self.labels[anchor_idx]\n",
    "        anchor_pdw = torch.FloatTensor(self.pdws[anchor_idx])\n",
    "\n",
    "        # --- Positive Selection ---\n",
    "        # Randomly select another sample from the same class\n",
    "        positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_pdw = torch.FloatTensor(self.pdws[positive_idx])\n",
    "\n",
    "        # --- Negative Selection ---\n",
    "        # Randomly select a sample from a different class\n",
    "        negative_label = random.choice([l for l in self.labels_set if l != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative_pdw = torch.FloatTensor(self.pdws[negative_idx])\n",
    "\n",
    "        return anchor_pdw, positive_pdw, negative_pdw\n",
    "\n",
    "# --- 4. Model Architecture (The Encoder f(x)) ---\n",
    "# An ANN/MLP-based network to process single PDWs and output an embedding.\n",
    "class EmitterEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ANN/MLP-based encoder to generate embeddings from single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(EmitterEncoder, self).__init__()\n",
    "        # A simple multi-layer perceptron\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.net(x)\n",
    "        # Normalize the embedding to have unit length (improves stability)\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "# --- 5. Triplet Loss Function (Removed custom class) ---\n",
    "# We will now use the built-in PyTorch loss function nn.TripletMarginLoss\n",
    "\n",
    "# --- 6. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # !!! IMPORTANT !!!\n",
    "    # !!! CHANGE THIS PATH TO POINT TO YOUR CSV FILE !!!\n",
    "    # CSV_FILE_PATH = 'sampledpoints.csv' \n",
    "    CSV_FILE_PATH = 'CombinedDeinterleaved.csv' \n",
    "    \n",
    "    print(\"--- Emitter Classification Training using Siamese Triplet Loss (ANN Version) ---\")\n",
    "\n",
    "    # Step 1: Load and Preprocess Data\n",
    "    try:\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(e)\n",
    "        # Create a dummy CSV file for demonstration if the specified one doesn't exist\n",
    "        print(f\"\\nCreating a dummy file '{CSV_FILE_PATH}' for demonstration purposes.\")\n",
    "        print(\"Please replace this with your actual data file.\")\n",
    "        dummy_data = {\n",
    "            'PW(µs)': np.random.rand(1000) * 10,\n",
    "            'Azimuth(º)': np.random.rand(1000) * 360,\n",
    "            'Elevation(º)': np.random.rand(1000) * 90,\n",
    "            'Power(dBm)': -50 + np.random.rand(1000) * 20,\n",
    "            'Freq(MHz)': 8000 + np.random.rand(1000) * 2000,\n",
    "            'Name': [random.choice(['Emitter_A', 'Emitter_B', 'Emitter_C']) for _ in range(1000)]\n",
    "        }\n",
    "        pd.DataFrame(dummy_data).to_csv(CSV_FILE_PATH, index=False)\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "\n",
    "    # Step 2: Split data into Training and Testing sets\n",
    "    print(f\"\\n2. Splitting data into {1-TEST_SPLIT_SIZE:.0%} training and {TEST_SPLIT_SIZE:.0%} testing...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pdws, labels, test_size=TEST_SPLIT_SIZE, random_state=42, stratify=labels\n",
    "    )\n",
    "    print(f\"   Training set size: {len(X_train)}\")\n",
    "    print(f\"   Testing set size: {len(X_test)}\")\n",
    "\n",
    "    # Step 3: Create Dataset and DataLoader for TRAINING data\n",
    "    print(\"\\n3. Creating Triplet Dataset and DataLoader for training...\")\n",
    "    train_dataset = TripletPDWDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Step 4: Initialize Model, Loss, and Optimizer\n",
    "    print(\"4. Initializing model, loss function, and optimizer...\")\n",
    "    input_dim = pdws.shape[1] \n",
    "    model = EmitterEncoder(input_dim=input_dim, hidden_dim=128, embedding_dim=EMBEDDING_DIM)\n",
    "    \n",
    "    criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Step 5: Training Loop with Loss Tracking\n",
    "    print(f\"5. Starting training for {EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            anchor_emb, positive_emb, negative_emb = model(anchor), model(positive), model(negative)\n",
    "            \n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"   Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training complete! ---\")\n",
    "\n",
    "    # Step 5.5: Plot Loss Curve\n",
    "    print(\"\\n5.5. Plotting training loss curve...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), epoch_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Training Loss Curve - Triplet Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Step 6: Evaluation using Mahalanobis Distance ---\n",
    "    print(\"\\n6. Evaluating the model on the test set using Mahalanobis distance...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # A: Generate embeddings for the entire training set\n",
    "        train_embeddings = model(torch.FloatTensor(X_train)).numpy()\n",
    "        \n",
    "        # B: Calculate the mean and inverse covariance matrix for each class distribution\n",
    "        print(\"   Calculating class distributions (mean and covariance)...\")\n",
    "        class_distributions = {}\n",
    "        for label in np.unique(y_train):\n",
    "            # Get all embeddings for the current class\n",
    "            class_embeddings = train_embeddings[y_train == label]\n",
    "            \n",
    "            # Calculate mean (centroid)\n",
    "            mean_vec = np.mean(class_embeddings, axis=0)\n",
    "            \n",
    "            # Calculate covariance matrix\n",
    "            # Add a small regularization term (identity matrix) for numerical stability before inverting\n",
    "            cov = np.cov(class_embeddings, rowvar=False)\n",
    "            reg_cov = cov + np.eye(class_embeddings.shape[1]) * 1e-6\n",
    "            \n",
    "            # Calculate the inverse of the regularized covariance matrix\n",
    "            inv_cov = np.linalg.inv(reg_cov)\n",
    "            \n",
    "            # Store the distribution parameters\n",
    "            class_distributions[label] = {'mean': mean_vec, 'inv_cov': inv_cov}\n",
    "            \n",
    "        # C: Generate embeddings for the test set\n",
    "        test_embeddings = model(torch.FloatTensor(X_test)).numpy()\n",
    "        \n",
    "        # D: Predict the label for each test embedding by finding the minimum Mahalanobis distance\n",
    "        predictions = []\n",
    "        for test_emb in test_embeddings:\n",
    "            distances = {}\n",
    "            for label, dist_params in class_distributions.items():\n",
    "                # Using scipy's mahalanobis distance which takes the inverse covariance matrix\n",
    "                distances[label] = mahalanobis(test_emb, dist_params['mean'], dist_params['inv_cov'])\n",
    "            \n",
    "            predicted_label = min(distances, key=distances.get)\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "        # E: Calculate and print accuracy and classification report\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"\\n   Overall Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        report_labels = sorted(list(label_mapping.keys()))\n",
    "        report_target_names = [label_mapping[lbl] for lbl in report_labels]\n",
    "        print(\"\\n   Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, labels=report_labels, target_names=report_target_names))\n",
    "\n",
    "\n",
    "    # Step 7: Visualization of TEST SET embeddings\n",
    "    print(\"\\n7. Visualizing the learned embedding space for the TEST SET using t-SNE...\")\n",
    "    # Use t-SNE to reduce dimensionality of test embeddings to 2D for plotting\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_test)-1), n_iter=1000)\n",
    "    embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Test Set Emitter Embeddings')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    legend_labels = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "    plt.legend(handles=scatter.legend_elements(num=len(legend_labels))[0], labels=legend_labels)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"--- Script finished. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac8742",
   "metadata": {},
   "source": [
    "### Experiment to directly cluster the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a920e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, v_measure_score\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# You can tune these parameters\n",
    "EMBEDDING_DIM = 128      # The size of the learned embedding vector\n",
    "MARGIN = 1.0             # The margin for the Triplet Loss\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "TEST_SPLIT_SIZE = 0.2    # 20% of data will be used for testing\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads PDW data from a CSV file and preprocesses it.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pdws_normalized (np.ndarray): Normalized array of PDW features.\n",
    "            - labels_numeric (np.ndarray): Array of numeric labels.\n",
    "            - label_mapping (dict): A dictionary mapping numeric labels back to original names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "        \n",
    "    print(f\"Loading data from '{file_path}'...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define feature and label columns\n",
    "    feature_columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)']\n",
    "    label_column = 'Name'\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in feature_columns + [label_column]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Error: CSV file must contain the column '{col}'.\")\n",
    "\n",
    "    # Extract features\n",
    "    pdws = df[feature_columns].values.astype(np.float32)\n",
    "\n",
    "    # Convert string labels to numeric labels\n",
    "    labels_numeric, unique_labels = pd.factorize(df[label_column])\n",
    "    label_mapping = {i: name for i, name in enumerate(unique_labels)}\n",
    "    \n",
    "    print(\"   Label mapping created:\")\n",
    "    for numeric_label, string_label in label_mapping.items():\n",
    "        print(f\"   {numeric_label} -> '{string_label}'\")\n",
    "\n",
    "    # Normalize all feature columns\n",
    "    pdws_normalized = np.copy(pdws)\n",
    "    for col in range(pdws_normalized.shape[1]):\n",
    "        mean = pdws_normalized[:, col].mean()\n",
    "        std = pdws_normalized[:, col].std()\n",
    "        if std > 0:\n",
    "            pdws_normalized[:, col] = (pdws_normalized[:, col] - mean) / std\n",
    "    \n",
    "    print(f\"   Successfully loaded and processed {len(df)} pulses.\")\n",
    "    return pdws_normalized, labels_numeric, label_mapping\n",
    "\n",
    "\n",
    "# --- 3. Triplet Dataset Class ---\n",
    "class TripletPDWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for generating triplets of single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdws, labels):\n",
    "        self.pdws = pdws\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.label_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "        if len(self.labels_set) < 2:\n",
    "            raise ValueError(\"The dataset must contain at least two different classes to create triplets.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pdws)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_idx = index\n",
    "        anchor_label = self.labels[anchor_idx]\n",
    "        anchor_pdw = torch.FloatTensor(self.pdws[anchor_idx])\n",
    "\n",
    "        positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_pdw = torch.FloatTensor(self.pdws[positive_idx])\n",
    "\n",
    "        negative_label = random.choice([l for l in self.labels_set if l != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative_pdw = torch.FloatTensor(self.pdws[negative_idx])\n",
    "\n",
    "        return anchor_pdw, positive_pdw, negative_pdw\n",
    "\n",
    "# --- 4. Model Architecture (The Encoder f(x)) ---\n",
    "class EmitterEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ANN/MLP-based encoder to generate embeddings from single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(EmitterEncoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.net(x)\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "# --- 6. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    CSV_FILE_PATH = 'CombinedDeinterleaved.csv' \n",
    "    \n",
    "    print(\"--- Emitter Classification Training using Siamese Triplet Loss (ANN Version) ---\")\n",
    "\n",
    "    # Step 1: Load and Preprocess Data\n",
    "    try:\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(e)\n",
    "        print(f\"\\nCreating a dummy file '{CSV_FILE_PATH}' for demonstration purposes.\")\n",
    "        dummy_data = {\n",
    "            'PW(µs)': np.random.rand(1000) * 10,\n",
    "            'Azimuth(º)': np.random.rand(1000) * 360,\n",
    "            'Elevation(º)': np.random.rand(1000) * 90,\n",
    "            'Power(dBm)': -50 + np.random.rand(1000) * 20,\n",
    "            'Freq(MHz)': 8000 + np.random.rand(1000) * 2000,\n",
    "            'Name': [random.choice(['Emitter_A', 'Emitter_B', 'Emitter_C']) for _ in range(1000)]\n",
    "        }\n",
    "        pd.DataFrame(dummy_data).to_csv(CSV_FILE_PATH, index=False)\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "\n",
    "    # Step 2: Split data into Training and Testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pdws, labels, test_size=TEST_SPLIT_SIZE, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Step 3: Create Dataset and DataLoader for TRAINING data\n",
    "    train_dataset = TripletPDWDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Step 4: Initialize Model, Loss, and Optimizer\n",
    "    input_dim = pdws.shape[1] \n",
    "    model = EmitterEncoder(input_dim=input_dim, hidden_dim=128, embedding_dim=EMBEDDING_DIM)\n",
    "    criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Step 5: Training Loop\n",
    "    print(f\"\\n5. Starting training for {EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            anchor_emb, positive_emb, negative_emb = model(anchor), model(positive), model(negative)\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"   Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"--- Training complete! ---\")\n",
    "\n",
    "    # Step 6: Generate Test Embeddings\n",
    "    print(\"\\n6. Generating embeddings for the test set...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model(torch.FloatTensor(X_test)).numpy()\n",
    "\n",
    "    # --- Step 7: EXPERIMENT - Unsupervised Clustering Evaluation ---\n",
    "    print(\"\\n7. EXPERIMENT: Evaluating embedding quality with K-Means clustering...\")\n",
    "    \n",
    "    # A: Determine the number of clusters (should be the number of unique emitters)\n",
    "    n_clusters = len(np.unique(y_test))\n",
    "    print(f\"   Running K-Means to find {n_clusters} clusters...\")\n",
    "    \n",
    "    # B: Run K-Means on the test embeddings\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(test_embeddings)\n",
    "    \n",
    "    # C: Evaluate the clustering quality against the true labels\n",
    "    ari = adjusted_rand_score(y_test, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(y_test, cluster_labels)\n",
    "    v_measure = v_measure_score(y_test, cluster_labels)\n",
    "    \n",
    "    print(\"\\n--- Clustering Evaluation Results ---\")\n",
    "    print(f\"   Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "    print(f\"   Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "    print(f\"   V-Measure: {v_measure:.4f}\")\n",
    "    print(\"   (Scores closer to 1.0 indicate better clustering that aligns with true labels)\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # Step 8: Visualization\n",
    "    print(\"\\n8. Visualizing the embedding space using t-SNE...\")\n",
    "    \n",
    "    # Reduce dimensionality for plotting\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_test)-1), n_iter=1000)\n",
    "    embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9))\n",
    "\n",
    "    # Plot 1: Colored by TRUE labels\n",
    "    scatter1 = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "    ax1.set_title('t-SNE Visualization (Colored by True Emitter Labels)')\n",
    "    ax1.set_xlabel('t-SNE Component 1')\n",
    "    ax1.set_ylabel('t-SNE Component 2')\n",
    "    legend_labels = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "    ax1.legend(handles=scatter1.legend_elements(num=len(legend_labels))[0], labels=legend_labels)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Colored by K-MEANS cluster labels\n",
    "    scatter2 = ax2.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    ax2.set_title('t-SNE Visualization (Colored by K-Means Cluster Labels)')\n",
    "    ax2.set_xlabel('t-SNE Component 1')\n",
    "    # ax2.set_ylabel('t-SNE Component 2') # Optional: remove for cleaner look\n",
    "    \n",
    "    # Create a legend for K-Means clusters\n",
    "    cluster_legend_labels = [f'Cluster {i}' for i in range(n_clusters)]\n",
    "    ax2.legend(handles=scatter2.legend_elements(num=n_clusters)[0], labels=cluster_legend_labels)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comparison of True Labels vs. Discovered K-Means Clusters', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"--- Script finished. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38270f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ebec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f47c2e7c",
   "metadata": {},
   "source": [
    "### Inference on unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6866244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "MODEL_SAVE_PATH = \"trained_model_full.pth\"\n",
    "STATS_SAVE_PATH = \"normalization_stats.pkl\"\n",
    "print(\"\\n\\n--- PHASE 2: Running Inference on Unlabeled Data ---\")\n",
    "feature_columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)']\n",
    "# Step 1: Initialize a new model instance for inference\n",
    "print(\"1. Initializing a new model for inference...\")\n",
    "inference_model = EmitterEncoder(input_dim=len(feature_columns), hidden_dim=128, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# Step 2: Load the saved weights\n",
    "inference_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "inference_model.eval() # Set to evaluation mode\n",
    "print(\"   Successfully loaded trained weights.\")\n",
    "\n",
    "df=pd.read_csv(\"CombinedDeinterleaved.csv\")\n",
    "### Normalisation stats\n",
    "pdws = df[feature_columns].values.astype(np.float32)\n",
    "labels_numeric, _ = pd.factorize(df['Name'])\n",
    "\n",
    "# Step 2: Normalize data and SAVE the normalization stats\n",
    "print(\"2. Normalizing data and saving stats for later use...\")\n",
    "pdws_normalized = np.copy(pdws)\n",
    "normalization_stats = {'mean': {}, 'std': {}}\n",
    "for col in range(pdws_normalized.shape[1]):\n",
    "    mean = pdws_normalized[:, col].mean()\n",
    "    std = pdws_normalized[:, col].std()\n",
    "    normalization_stats['mean'][col] = mean\n",
    "    normalization_stats['std'][col] = std\n",
    "    if std > 0:\n",
    "        pdws_normalized[:, col] = (pdws_normalized[:, col] - mean) / std\n",
    "\n",
    "with open(STATS_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(normalization_stats, f)\n",
    "print(f\"   Normalization stats saved to '{STATS_SAVE_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef021b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(STATS_SAVE_PATH, 'rb') as f:\n",
    "    loaded_stats = pickle.load(f)\n",
    "print(\"   Loaded normalization stats from training phase.\")\n",
    "\n",
    "\n",
    "unlabeled_pdws=pd.read_csv(\"Set4.csv\")\n",
    "unlabeled_pdws=unlabeled_pdws[feature_columns]\n",
    "# Step 4: Normalize the new data using the LOADED stats\n",
    "unlabeled_pdws_normalized = np.copy(unlabeled_pdws)\n",
    "for col in range(unlabeled_pdws_normalized.shape[1]):\n",
    "    mean = loaded_stats['mean'][col]\n",
    "    std = loaded_stats['std'][col]\n",
    "    if std > 0:\n",
    "        unlabeled_pdws_normalized[:, col] = (unlabeled_pdws_normalized[:, col] - mean) / std\n",
    "\n",
    "# Step 5: Generate embeddings for the unlabeled data\n",
    "print(\"3. Generating embeddings for the new data...\")\n",
    "with torch.no_grad():\n",
    "    unlabeled_embeddings = inference_model(torch.FloatTensor(unlabeled_pdws_normalized)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Cluster the embeddings to find the 9 known emitter types\n",
    "N_CLUSTERS = 9\n",
    "UNLABELED_DATA_PATH='Set4.csv'\n",
    "print(f\"4. Running K-Means to find {N_CLUSTERS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(unlabeled_embeddings)\n",
    "\n",
    "# Step 7: Visualize the results\n",
    "print(\"5. Visualizing the embedding space...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(unlabeled_embeddings)-1))\n",
    "embeddings_2d = tsne.fit_transform(unlabeled_embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title(f't-SNE Visualization of Unlabeled Data from {UNLABELED_DATA_PATH}')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(handles=scatter.legend_elements(num=N_CLUSTERS)[0], labels=[f'Discovered Cluster {i}' for i in range(N_CLUSTERS)])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a78ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Step 6: Use Gaussian Mixture Model with BIC to find the optimal number of clusters\n",
    "print(\"4. Running Gaussian Mixture Model (GMM) with BIC to find optimal clusters...\")\n",
    "\n",
    "# Define a range of potential cluster numbers to test\n",
    "n_components_range = range(2, 16) # Test from 2 to 15 clusters\n",
    "\n",
    "# Calculate BIC for each number of components\n",
    "bic_scores = []\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42, n_init=10)\n",
    "    gmm.fit(unlabeled_embeddings)\n",
    "    bic_scores.append(gmm.bic(unlabeled_embeddings))\n",
    "    print(f\"   BIC for {n_components} components: {bic_scores[-1]:.2f}\")\n",
    "\n",
    "# Find the optimal number of clusters (the one with the lowest BIC)\n",
    "optimal_n_clusters = n_components_range[np.argmin(bic_scores)]\n",
    "print(f\"\\n   Optimal number of clusters found via BIC: {optimal_n_clusters}\")\n",
    "\n",
    "# Fit the final GMM with the optimal number of clusters\n",
    "best_gmm = GaussianMixture(n_components=optimal_n_clusters, random_state=42, n_init=10)\n",
    "best_gmm.fit(unlabeled_embeddings)\n",
    "cluster_labels = best_gmm.predict(unlabeled_embeddings)\n",
    "\n",
    "# Step 7: Visualize the results\n",
    "print(\"5. Visualizing the embedding space...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(unlabeled_embeddings)-1))\n",
    "embeddings_2d = tsne.fit_transform(unlabeled_embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title(f't-SNE Visualization using GMM (Found {optimal_n_clusters} Clusters)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(handles=scatter.legend_elements(num=optimal_n_clusters)[0], labels=[f'Discovered Cluster {i}' for i in range(optimal_n_clusters)])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba18c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# You can tune these parameters\n",
    "EMBEDDING_DIM = 128     # The size of the learned embedding vector\n",
    "MARGIN = 1.0            # The margin for the Triplet Loss\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "TEST_SPLIT_SIZE = 0.2   # 20% of data will be used for testing\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads PDW data from a CSV file and preprocesses it.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pdws_normalized (np.ndarray): Normalized array of PDW features.\n",
    "            - labels_numeric (np.ndarray): Array of numeric labels.\n",
    "            - label_mapping (dict): A dictionary mapping numeric labels back to original names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "        \n",
    "    print(f\"Loading data from '{file_path}'...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define feature and label columns\n",
    "    feature_columns = ['PW(µs)','Azimuth(º)','Elevation(º)','Power(dBm)','Freq(MHz)']\n",
    "    label_column = 'Name'\n",
    "\n",
    "    # Check for required columns\n",
    "    for col in feature_columns + [label_column]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Error: CSV file must contain the column '{col}'.\")\n",
    "\n",
    "    # Extract features\n",
    "    pdws = df[feature_columns].values.astype(np.float32)\n",
    "\n",
    "    # Convert string labels to numeric labels\n",
    "    labels_numeric, unique_labels = pd.factorize(df[label_column])\n",
    "    label_mapping = {i: name for i, name in enumerate(unique_labels)}\n",
    "    \n",
    "    print(\"   Label mapping created:\")\n",
    "    for numeric_label, string_label in label_mapping.items():\n",
    "        print(f\"   {numeric_label} -> '{string_label}'\")\n",
    "\n",
    "    # Normalize all feature columns\n",
    "    pdws_normalized = np.copy(pdws)\n",
    "    for col in range(pdws_normalized.shape[1]):\n",
    "        mean = pdws_normalized[:, col].mean()\n",
    "        std = pdws_normalized[:, col].std()\n",
    "        if std > 0:\n",
    "            pdws_normalized[:, col] = (pdws_normalized[:, col] - mean) / std\n",
    "    \n",
    "    print(f\"   Successfully loaded and processed {len(df)} pulses.\")\n",
    "    return pdws_normalized, labels_numeric, label_mapping\n",
    "\n",
    "\n",
    "# --- 3. Triplet Dataset Class ---\n",
    "# This custom PyTorch Dataset creates triplets of single PDW vectors on-the-fly.\n",
    "class TripletPDWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for generating triplets of single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdws, labels):\n",
    "        self.pdws = pdws\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Group indices by label for efficient triplet mining\n",
    "        self.label_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "        if len(self.labels_set) < 2:\n",
    "            raise ValueError(\"The dataset must contain at least two different classes to create triplets.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pdws)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # --- Anchor Selection ---\n",
    "        anchor_idx = index\n",
    "        anchor_label = self.labels[anchor_idx]\n",
    "        anchor_pdw = torch.FloatTensor(self.pdws[anchor_idx])\n",
    "\n",
    "        # --- Positive Selection ---\n",
    "        # Randomly select another sample from the same class\n",
    "        positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_pdw = torch.FloatTensor(self.pdws[positive_idx])\n",
    "\n",
    "        # --- Negative Selection ---\n",
    "        # Randomly select a sample from a different class\n",
    "        negative_label = random.choice([l for l in self.labels_set if l != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative_pdw = torch.FloatTensor(self.pdws[negative_idx])\n",
    "\n",
    "        return anchor_pdw, positive_pdw, negative_pdw\n",
    "\n",
    "# --- 4. Model Architecture (The Encoder f(x)) ---\n",
    "# An ANN/MLP-based network to process single PDWs and output an embedding.\n",
    "class EmitterEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ANN/MLP-based encoder to generate embeddings from single PDW vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(EmitterEncoder, self).__init__()\n",
    "        # A simple multi-layer perceptron\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.net(x)\n",
    "        # Normalize the embedding to have unit length (improves stability)\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "# --- 5. Triplet Loss Function ---\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Calculate Euclidean distances\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # d(a, p)^2\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # d(a, n)^2\n",
    "        # Calculate loss\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "# --- 6. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # !!! IMPORTANT !!!\n",
    "    # !!! CHANGE THIS PATH TO POINT TO YOUR CSV FILE !!!\n",
    "    # CSV_FILE_PATH = 'sampledpoints.csv' \n",
    "    CSV_FILE_PATH = 'CombinedDeinterleaved.csv' \n",
    "    \n",
    "    print(\"--- Emitter Classification Training using Siamese Triplet Loss (ANN Version) ---\")\n",
    "\n",
    "    # Step 1: Load and Preprocess Data\n",
    "    try:\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(e)\n",
    "        # Create a dummy CSV file for demonstration if the specified one doesn't exist\n",
    "        print(f\"\\nCreating a dummy file '{CSV_FILE_PATH}' for demonstration purposes.\")\n",
    "        print(\"Please replace this with your actual data file.\")\n",
    "        dummy_data = {\n",
    "            'AOA': np.random.rand(1000) * 360,\n",
    "            'PW': np.random.rand(1000) * 10e-6,\n",
    "            'Frequency': 8e9 + np.random.rand(1000) * 1e9,\n",
    "            'Amplitude': 50 + np.random.rand(1000) * 10,\n",
    "            'Name': [random.choice(['Emitter_A', 'Emitter_B', 'Emitter_C']) for _ in range(1000)]\n",
    "        }\n",
    "        pd.DataFrame(dummy_data).to_csv(CSV_FILE_PATH, index=False)\n",
    "        pdws, labels, label_mapping = load_and_preprocess_data(CSV_FILE_PATH)\n",
    "\n",
    "    # Step 2: Split data into Training and Testing sets\n",
    "    print(f\"\\n2. Splitting data into {1-TEST_SPLIT_SIZE:.0%} training and {TEST_SPLIT_SIZE:.0%} testing...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pdws, labels, test_size=TEST_SPLIT_SIZE, random_state=42, stratify=labels\n",
    "    )\n",
    "    print(f\"   Training set size: {len(X_train)}\")\n",
    "    print(f\"   Testing set size: {len(X_test)}\")\n",
    "\n",
    "    # Step 3: Create Dataset and DataLoader for TRAINING data\n",
    "    print(\"\\n3. Creating Triplet Dataset and DataLoader for training...\")\n",
    "    train_dataset = TripletPDWDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Step 4: Initialize Model, Loss, and Optimizer\n",
    "    print(\"4. Initializing model, loss function, and optimizer...\")\n",
    "    input_dim = pdws.shape[1] \n",
    "    model = EmitterEncoder(input_dim=input_dim, hidden_dim=128, embedding_dim=EMBEDDING_DIM)\n",
    "    criterion = TripletLoss(margin=MARGIN)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Step 5: Training Loop with Loss Tracking\n",
    "    print(f\"5. Starting training for {EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize loss tracking\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            anchor_emb, positive_emb, negative_emb = model(anchor), model(positive), model(negative)\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"   Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training complete! ---\")\n",
    "\n",
    "    # Step 5.5: Plot Loss Curve\n",
    "    print(\"\\n5.5. Plotting training loss curve...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), epoch_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Training Loss Curve - Triplet Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Step 6: Evaluation\n",
    "    print(\"\\n6. Evaluating the model on the test set...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # A: Generate embeddings for the entire training set to define cluster centroids\n",
    "        train_embeddings = model(torch.FloatTensor(X_train)).numpy()\n",
    "        \n",
    "        # B: Calculate the centroid for each cluster from the training data\n",
    "        cluster_centroids = defaultdict(list)\n",
    "        for i, label in enumerate(y_train):\n",
    "            cluster_centroids[label].append(train_embeddings[i])\n",
    "        \n",
    "        for label in cluster_centroids:\n",
    "            cluster_centroids[label] = np.mean(cluster_centroids[label], axis=0)\n",
    "            \n",
    "        # C: Generate embeddings for the test set\n",
    "        test_embeddings = model(torch.FloatTensor(X_test)).numpy()\n",
    "        \n",
    "        # D: Predict the label for each test embedding by finding the closest centroid\n",
    "        predictions = []\n",
    "        for test_emb in test_embeddings:\n",
    "            distances = {label: np.linalg.norm(test_emb - centroid) for label, centroid in cluster_centroids.items()}\n",
    "            predicted_label = min(distances, key=distances.get)\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "        # E: Calculate and print accuracy and classification report\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"\\n   Overall Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        report_labels = sorted(list(label_mapping.keys()))\n",
    "        report_target_names = [label_mapping[lbl] for lbl in report_labels]\n",
    "        print(\"\\n   Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, labels=report_labels, target_names=report_target_names))\n",
    "\n",
    "\n",
    "    # Step 7: Visualization of TEST SET embeddings\n",
    "    print(\"\\n7. Visualizing the learned embedding space for the TEST SET using t-SNE...\")\n",
    "    # Use t-SNE to reduce dimensionality of test embeddings to 2D for plotting\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_test)-1), n_iter=1000)\n",
    "    embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Test Set Emitter Embeddings')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    legend_labels = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "    plt.legend(handles=scatter.legend_elements(num=len(legend_labels))[0], labels=legend_labels)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"--- Script finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (harshenv)",
   "language": "python",
   "name": "harshenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
